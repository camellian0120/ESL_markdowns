# 3.2. 線形回帰モデルと最小2乗法
線形回帰モデル ... 以下の式で表されるモデル。回帰関数$E(Y|X)$がおおよそ線形であることを仮定している。
$$f(X) = β_0 + \sum^p_{j=1} X_j β_j$$
この時、線形回帰モデルの$β_j$が未知のパラメータか係数である。\
また、変数$X_j$は以下の様な量を表している。
1. 量的変数
2. 量的変数を対数、平方根、2乗などに変換したもの
3. $X_2 = X_1^2$、$X_3 = X_1^3$の様な多項式表現に繋がる基底展開
4. 質的変数を表す記号や符号 (ダミー変数)
5. $X_3 = X_1 ・ X_2$のような変数間の関係

学習に際しては、訓練データ$(x_n, y_n)$からパラメータ$β$を推定する。\
最も一般的な手法としては、最小2乗法である。\
残差平方和$RSS$を最小にするような$β = (β_0, β_1, ..., β_p)^T$を求めれば良い。
$$RSS(β) = \sum^N_{i=1}(y_i - f(x_i))^2 = \sum^N_{i=1}(y_i - β_0 - \sum^p_{j=q} {x_{ij} β_j})^2$$
上記の$RSS$を最小化するためには、最適なパラメータを推定する必要がある。\
そのために、推定値$\hat y = X \hat β$を算出すればよい。\
回帰分析ソフトでは、一意解が求まらない(ランク落ちが発生する)ような場合でもフィルタリングや正則化によって制御する。

## 3.2.2 ガウス=マルコフ定理
パラメータ$β$の最小2乗推定は、線形の推定量の中で最小の分散を持っている。\
これは、普遍推定量以外でも利用することができる。\
平均2乗誤差$MSE$より、
$$MSE(\tilde θ) = Var(\tilde θ) - [E(\tilde θ-θ)]^2$$
これは、第1項が分散であり、第2項はバイアスの2乗である。
正規分布の仮定の下で、パラメータの推定と信頼区間を計算している。\
バイアスと分散との間には、トレードオフが存在する。

## 3.2.3 重回帰分析へ
多重線形回帰モデルは、単回帰モデルでの推定を考えると理解が容易になる。
- 多重線形回帰 ... 複数の独立な説明変数から1つの目的変数を予測すること
- 単回帰 ... 1つの説明変数から1つの目的変数を予測すること

相関がある入力が多い場合は、残差ベクトル$z_p$が$0$に近づいていき、係数$\hat β$も不安定になる。\
この際は、Z-スコアやF検定量を元に不要な変数を除去する必要があるが、これが難しい場合がある。
- Z-スコア ... データの分布にも基づいて、あるデータ点が平均からどれだけ離れているかを標準偏差の単位で示すもの\
 Z-スコアが1なら標準偏差1つ分だけ離れていることがわかる。データの標準化や異常値の特定に利用できる。
- f検定 ... 2つの母分散が等しいかどうかを判別する、等分散性の検定である。

グラム=シュミットの直行化法は、推定値を計算する便利な解法である。
1. 初期化 $z_0 = x_0 = 1$
2. $j=1,2,...,p$に対して、\
 係数$\hat \gamma_{lj} = \langle z_l, x_j \rangle / \langle z_l, z_l \rangle (l=0, ..., j-1)$と\
 残差ベクトル$z_j = x_j - \sum^{j-1}_{k=0} {\hat \gamma_{kj} z_k}$を求める為に、$x_j$の$z_0, z_1, ..., z_{j-1}$への回帰を行う。
3. $y$への残差$z_p$への回帰を行い、推定値$\hat β_p$を得る。

## 3.2.4 複数の目的変数
$N$個の目的変数がある場合は、行列表現を使って、
$$Y=XB+E$$
となる。\
ここでは、$Y$は$N×K$目的変数行列、$X$は$N×(p+1)$説明変数行列、$B$は$(p+1)×K$係数行列、$E$は$N×K$誤差行列である。
残差平方和$RSS$をそのまま適応することができる。
この時、行列表現内に内包される誤差が相関を持っているのならば、$RSS$を複数の目的変数に合うように変更することが妥当であり、特に共分散行列$Cov(\epsilon) = \Sigma$なら、
$$RSS(B; E) = \sum^N_{i=1} {(y_i - f(x_i))^T \Sigma^{-1} (y_i - f(x_i))}$$

# 3.3. 変数選択
最小2乗推定で満足できない理由は2つある。
- 1つ目の理由は予測精度である。バイアスを多少犠牲にすることで、予測値の分散を減少させて、予測政府度を向上させたいものである。
- 2つ目の理由は解釈である。全体を見るために細かい変数を犠牲にしたい際に不便である。

## 3.3.1 最良変数組み合わせ
最良部分集合による回帰では、最も小さい残差2乗和$RSS$が得られる大きさ$k$の部分集合をあらゆる0以上の自然数から求める。\
だが、効率的なアルゴリズムである跳躍限定法を使っても、高々30~40程度の値までしか扱うことができない。
決定の際には、期待予測誤差を最小化する最も小さいモデルを使用する。

## 3.3.2 漸次的選択法


## 3.3.3 前向き段階回帰


# 3.4 縮小推定
変数選択では、予測変数の一部だけを用いて残りをモデルから除外することで、全変数を用いたモデルより予測誤差が小さく、理解しやすいモデルが作成可能になる。\
その一方で、分散が大きくなり予測誤差を小さくすることが難しいことが知られている。

## 3.4.1 リッジ回帰
リッジ回帰とは、重回帰分析の損失関数に正則化項を加えたものである。\
この時、罰則付き残差2乗和を最小化することで回帰係数を求めることができる。\
以下の式の最後の項が正則化項であり、リッジ回帰ではL1ノルムが利用されている。
$$\hat β^{ridge} = \underset{β} {argmin}　\{\sum^N_{i=1}(y_i - β_0 - \sum^p_{j=1} x_{ij} β_j)^2 + λ\sum^p_{j=1}β^2_j \}$$

- 荷重減衰 ... パラメータの2乗和によって罰則を付加するという考え方。
- 中心化 ... 各入力変数$X$を入力変数の平均$\hat X$を引くことで元の値の大小による解釈性の悪化を防ぐことができる
- 有効自由度 ... 自由に値を取れるデータ数のこと。
- 特異値分解 ... 
- 固有値分解 ... 
- 主成分 ... 

## 3.4.2 lasso
lassoも、重回帰分析の損失関数に正則化項を加えたものである。


基底追跡

## 3.4.3 部分集合選択 or リッジ回帰 or lasso
リッジ回帰とlassoの折衷案として、エラスティックネットが存在する。\
エラスティックネットでは、損失関数に加えて、L1及びL2正則化項をどちらも足し合わせた形になっている。

単調増加かつ外挿性能が求められる場合には有効な手法

## 3.4.4 最小角回帰

