# 2.1. 教師あり学習の概要
**教師あり学習**とは、入力された変数から出力の変数を予測することである。\
教師あり学習の変数には以下の2種類がある。
- **応答変数 (response variable)** … 予測した結果として出力される変数。
- **予測変数 (predictor)** … 応答変数を予測するために使われる変数。

# 2.2. 変数の種類と用語
変数全体として、離散値を取る**離散型**の変数と連続値を取る**連続型**の変数が存在する。\
その中で、出力変数にもいくつかの種類があり、そのうちの4つを以下に示す。
1. **量的変数 (quantative variable)** … 数値で表すことができ、大小関係が存在する変数のこと。\
 例) 年齢、身長、体重 etc
2. **質的変数 (quanlitative variable)** … 数値の大小に意味はなく、種類や分類を区別するための変数。\
 例) 名前、血液型、職業 etc 
3. **カテゴリ型変数 (categorival variable)** … 質的変数を量的変数に置き換えた変数。\
 例) {女性, 男性} -> {\-1, 1} etc
4. **順序付きカテゴリ型変数** … 値の順序に意味はあるが、計算としての意味は存在しない変数。\
 例) {大, 中, 小}

また、質的変数を数値として表現する際の方法としては以下の2つのようなものがある。
1. **目標変数** … 2つのカテゴリを**0と1**または<strong>-1と1</strong>のように2値を使って表す方法
2. **ダミー変数** … 3つ以上のカテゴリがある場合は、**カテゴリ毎の変数**を用意して、カテゴリに**当てはまる場合は1**、**そうでない場合は0**という風に数値を当てはめる方法である。

予測問題にもいくつかの種類があり、出力の違いによって名称が異なる。
1. **回帰** … 量的変数を予測する問題。
2. **分類** … 質的変数を予測する問題。

また、予測のための規則を生成するために使われるデータを**訓練データ**といい、予測変数を出力する学習器をデータを元に訓練することを学習という。

# 2.3.1. 最小2乗法
線形モデルでは、入力ベクトル$X^{T} = (X_1, X_2, ..., X_p)$が与えられたとき、出力$Y$を、
$$\hat{Y}=\hat{β_0}+\sum^{p}_{j=1}X_{j}\hat{β_j}$$
と予測する。
このとき、$β_0$は切片であり、**バイアス**とも呼ばれることがある。\
また、$X_0=1$とすると表記を簡潔にすることができ、
$$\hat{Y}=X^{T}\hat{β}$$
の様な内積の形に変形できる。\
次に、線形モデルに訓練データを当てはめる方法を考える。\
最も有名な方法として、**最小2乗法**がある。残差2乗和
$$RSS(β) = \sum^{N}_{i=1}(\hat{y_i} - f(x_i^T β))^2$$
を最小にする$β$を選択する。(この時、2次関数であるので、必ず最小値は存在する) 行列表記にすると、
$$RSS(β) = (y - Xβ)^T (y - Xβ)$$
と書ける。この時$X$は$N×p$ 行列なので、$y$は入力の個数によらず$N$次元ベクトルである。\
これを、$β$に関して偏微分すると、**正規方程式 (normal equation)**
$$X^T (y - Xβ) =0$$
が得られ、$X^T X$が特異でなければ、正規方程式の一意な解を
$$\hat{β} = (X^T X)^{-1}X^T y$$
と求められる。これにより、任意の入力$x_0$に対して$x_0^T \hat{β}$が予測値となる。

また、分類問題において応答変数の値が変化する境界を**決定境界(decision boundary)**という。\
しかし、線形モデルでは全てが正しく分類されるわけではなく、柔軟性の欠如や本質的に避けられないノイズによって誤分類されることもある。

# 2.3.2. 最近傍法
a

# 2.3.3. 最小2乗法から最近傍法へ
a

# 2.4. 統計的決定理論
a

# 2.5. 高次元での局所的手法
a
