# 3.5. 入力に対して線形変換を行う方法
入力変数が多く、互いに高い相関を持つ際の手法を考える。\
元の入力$X_j$に対して、少数の線型結合$Z_m (m = 1, ..., M)$を計算し、計算結果を入力変数の代わりとして利用する。\
この節では、主成分回帰と部分最小2乗法を扱う。

## 3.5.1 主成分回帰 (Principal Component Regression)
主成分回帰は、以下の流れの繰り返しである。
1. 説明変数のうち、相関のある変数群の線形結合$Z_m$を作り、これを主成分と呼ぶ。 (= **主成分分析**)
2. 主成分のうち、分散の大きい主成分を選ぶ。
3. 累積寄与率が一定値を超えるまで選択し、これを新たな説明変数とする。

入力の主成分を使用していたり、入力変数の大きさに対して不変でないため標準化が必要だったりといった点でリッジ回帰に似ている。
- 主成分分析 ... 相関のある多数の変数の中から、相関のない少数で全体のバラツキをよく表す**主成分**と呼ばれる変数を取り出してくる操作

主成分回帰では、入力列ベクトル$z_m = X v_m$を計算して得られた$z_1, ..., z_M (M<P)$を用いて$y$の回帰分析を行う。$z_m$は直交するため、単回帰の総和であると考えられる。
$$\hat y^{pcr}_{(M)} = \bar y 1 + \sum^M_{m=1} {\hat θ_m z_m} 　 (\hat θ = \langle z_m, y \rangle / \langle z_m, z_m \rangle) \tag{3.61}$$

$z_m$が入力$x_j$の線形結合なので、(3.61)の解を前述の$\hat θ$及び固有ベクトル$v_m$を使って、以下の様に表せる。
$$\hat \beta (M) = \sum^M_{m=1} {\hat θ_m v_m} \tag{3.62}$$

PCRのメリット
1. 多重共線性の解消 ... 相関のある入力変数を1つの主成分として扱うため、変数間の相関を無くすことができる。
2. 次元削減による過学習の防止 ... 次元数が多いほど、訓練データのノイズにまでフィットしてしまうため。

PCRのデメリット
1. 変数の解釈性の低下 ... 主成分は変数の相関のみを元に和を取るため、意味合いを考えることが難しくなる。
2. 予測性能低下の可能性 ... 主成分が目的変数$y$を無視して作られるため、必ずしも予測性能が高くなるとは限らない。
3. 乗法損失のリスク

PCRの注意点
- 主成分の数$M = $ 元の変数の数$p$の際には通常の最小2乗推定を行っているのと変わらない。

## 3.5.2 部分最小2乗法 (Partial Least Squares Regression)
部分最小2乗法は、以下の流れの繰り返しである。
1. 各次元において、**説明変数と目的変数の内積**$\hat \varphi_{1j} = \langle x_j, y \rangle$を計算する。
2. 計算した$\hat \varphi_{1j}$を元に、入力$z_1 = \sum_j \hat \varphi_{1j} x_j$を計算する。
3. 

PLSのメリット
1. 多重共線性の解消
2. 予測性能 ... 説明変数と目的変数の相関を元に共分散を最大化する潜在変数を選択するため、予測性能が高くなることが多い。

- 潜在変数 ... 直接観測されないが、観測された他の変数から推定される変数

PLSのデメリット
1. a

# 3.6 考察

# 3.7. 複数の目的変数の縮小推定と変数選択 (Munch)

# 3.8. lassoと関連すつ解追跡アルゴリズムに関する詳細

## 3.8.1. 逐次前向き段階的回帰

## 3.8.2. 区分的線形解追跡アルゴリズム

## 3.8.3. ダンツィク選択器

## 3.8.4. グループlasso

## 3.8.5. lassoの性質について

## 3.8.6 総当り座標最適化

# 3.9. 計算上考慮すべき事柄

# 参考文献
[1]主成分分析 - Wikipedia (参照:2025/05/08)
https://ja.wikipedia.org/wiki/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90

[2]