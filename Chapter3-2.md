# 3.5. 入力に対して線形変換を行う方法
入力変数が多く、互いに高い相関を持つ際の手法を考える。\
元の入力$X_j$に対して、少数の線型結合$Z_m (m = 1, ..., M)$を計算し、計算結果を入力変数の代わりとして利用する。\
この節では、主成分回帰と部分最小2乗法を扱う。

## 3.5.1 主成分回帰 (Principal Component Regression)
主成分の線型結合$Z_m$や**主成分分析**を利用する。
- 主成分分析 ... 相関のある多数の変数の中から、相関のない少数で全体のバラツキをよく表す**主成分**と呼ばれる変数を取り出してくる操作

主成分回帰では、入力列ベクトル$z_m = X v_m$を計算して得られた$z_1, ..., z_M (M<P)$を用いて$y$の回帰分析を行う。
$$\hat y^{pcr}_{(M)} = \bar y 1 + \sum^M_{m=1} {\hat θ_m z_m}$$


$$\hat \beta (M) = \sum^M_{m=1} {\hat θ_m v_m}$$

PCRのメリットとしては、以下の3つのものがある。
1. 多重線形成の解消
2. 次元削減による過学習の防止
3. 視覚的・構造的な理解がしやすい

PCRの注意点は、主成分が目的変数$y$を無視して作られるので、必ずしも予測性能が高くなるとは限らない。

## 3.5.2 部分最小2乗法

# 3.6 考察

# 3.7. 複数の目的変数の縮小推定と変数選択 (Munch)

# 3.8. lassoと関連すつ解追跡アルゴリズムに関する詳細

## 3.8.1. 逐次前向き段階的回帰

## 3.8.2. 区分的線形解追跡アルゴリズム

## 3.8.3. ダンツィク洗濯機

## 3.8.4. グループlasso

## 3.8.5. lassoの性質について

## 3.8.6 総当り座標最適化

# 3.9. 計算上考慮すべき事柄

# 参考文献
[1]主成分分析 - Wikipedia (参照:2025/05/08)
https://ja.wikipedia.org/wiki/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90

[2]